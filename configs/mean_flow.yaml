data:
  dataset: "celeba"
  root: "./data/celeba"   # writable path (use /data/celeba on clusters if available)
  from_hub: true
  repo_name: "electronickale/cmu-10799-celeba64-subset"
  image_size: 64
  channels: 3
  num_workers: 4
  pin_memory: true
  augment: true

model:
  base_channels: 128
  channel_mult: [1, 2, 2, 4]
  num_res_blocks: 2
  attention_resolutions: [8, 16]
  num_heads: 4
  dropout: 0.1
  use_scale_shift_norm: true

# MeanFlow-specific: (r,t) sampling and loss
method:
  r_t_sampler: "logit_normal"  # "uniform" or "logit_normal"
  logit_normal_mu: -0.4
  logit_normal_sigma: 1.0
  pct_r_neq_t: 0.25           # fraction of steps with r != t (rest is Flow Matching)
  loss_adaptive_p: 1.0        # 0 = plain MSE
  loss_adaptive_c: 1.0e-3

training:
  batch_size: 64   # smaller than flow_matching: JVP doubles memory use; use 32 if OOM on your GPU
  learning_rate: 2e-4
  weight_decay: 0.0
  betas: [0.9, 0.999]
  ema_decay: 0.999
  ema_start: 5000
  gradient_clip_norm: 1.0
  num_iterations: 40000
  warmup_iters: 5000
  log_every: 100
  sample_every: 1000
  save_every: 5000
  num_samples: 16

sampling:
  num_steps: 1    # 1-NFE by default; use 2-4 for few-step fallback
  sampler: "meanflow"

infrastructure:
  seed: 42
  device: "cuda"
  num_gpus: 1
  mixed_precision: true
  compile_model: false
  # If OOM on large GPU: try batch_size 32 and/or PYTORCH_ALLOC_CONF=expandable_segments:True

checkpoint:
  dir: "./checkpoints"
  resume: true

logging:
  dir: "./logs"
  wandb:
    enabled: true
    project: "cmu-10799-diffusion"
    entity: null
