\hwpart{I}{Understanding Your Data (10 points)}
Since the goal for generative modeling is to model the distribution of your data, it is important to first understand what does this distribution look like.

The dataset for this course is a custom subset of CelebA~\citep{liu2015faceattributes}, filtered to have specific properties. Your first task is to discover what those properties are.

\question{Visual Exploration}{5}

\subq{a}{2} Visualize a grid of at least 16 random samples from the training set. Include this grid below.

\answerbox{}{
    \centering
    % Direct include to avoid path/environment issues
    \includegraphics[width=0.6\textwidth]{figures/celeba_samples_4x4.png} 
    \vspace{0.5cm}
}

\subq{b}{3} This dataset was filtered from full CelebA using their 40 binary attributes. Based on your visual exploration, hypothesize which attributes were likely used to create this subset. What filtering criteria were used to create this subset? Why?

\answerbox{}{
    Based on the visual exploration, I hypothesize that this subset was created by filtering for \textbf{unobstructed faces}. The specific criteria used were likely:
    \begin{itemize}
        \item \texttt{Eyeglasses == 0} (Absent)
        \item \texttt{Wearing\_Hat == 0} (Absent)
        \item \texttt{No\_Beard == 1} (Present/Clean Shaven)
    \end{itemize}
    
    \textbf{Why?} These attributes represent significant occlusions or variable geometries that hide key facial landmarks. Removing them simplifies the data manifold, allowing the generative model to focus on learning core facial features (skin, eyes, structure) without needing to model complex object physics (glasses reflections) or high-variance textures (heavy beards). This generally stabilizes training.
}

\question{What did you learn?}{5}

\subq{a}{3} Imagine you found a pre-trained diffusion model that can reach SOTA performance on full CelebA (200K diverse faces). Now you generated samples and compared them to this filtered subset using KID or FID. Would you expect the FID and KID score to still be SOTA? Why?

\answerbox{}{
    \textbf{No, I would not expect the scores to be SOTA.}
    
    FID and KID measure the distance between the distribution of the \textit{generated} images and the \textit{reference} dataset. In this case, the reference is the \textbf{filtered subset} (no hats/glasses/beards), while the generator (trained on full CelebA) will produce images containing hats, sunglasses, and beards.
    
    Even if the image quality is perfect, the Inception network will detect these "out-of-distribution" features in the generated batch. This domain mismatch will cause the feature statistics (mean and covariance) to diverge significantly from the reference data, resulting in a poor i.e. high FID/KID score.
}

\subq{b}{2} Based on your exploration, what kind of data augmentation transformation do you plan to use for your training and why?

\answerbox{}{
    I plan to use \textbf{Random Horizontal Flips} across vertical axis.
    
    This is because Human faces exhibit rough bilateral symmetry. Flipping a face horizontally creates a valid, realistic training sample that is semantically equivalent to the original, effectively doubling the dataset size. This can even prevent potential biases such as an image being either centred or angled at left profile for example without augmentation.
    
    I would avoid transformations like vertical flips across horizontal axis (faces aren't upside down) or extreme rotations, as CelebA is likely already centered and aligned. Breaking this alignment would make the generative task significantly harder for the model.
}