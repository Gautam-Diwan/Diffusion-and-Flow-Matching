
\hwpart{IV}{Ablation Study (30 points)}
In this part of the homework, we will explore different design choices of the DDPM algorithm and how they can affect the final performance.

\question{Alternative Parametrization}{20}

\subq{a}{7} The original DDPM paper parametrized the model to predict the noise $\epsilon$. However, this is not the only option! First derive an alternative parametrization for the same model (i.e. what else can the model predict that can still yield the same mathematical formulation of the algorithm).

\answerbox{}{
% Your answer here
The original DDPM predicts noise $\epsilon$. However, we can derive alternative parametrizations by starting from the forward process:

$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$

\textbf{Alternative Parametrization 1: Predicting $x_0$ directly}

Rearranging the forward process to solve for $x_0$:

$$x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}$$

The model can learn to predict $\hat{x}_0(x_t, t)$ directly instead of $\hat{\epsilon}(x_t, t)$. The loss becomes:

$$\mathcal{L} = \mathbb{E}_{x_0, \epsilon, t} \left[ \|\hat{x}_0(x_t, t) - x_0\|^2 \right]$$

\textbf{Why this works:} Given the forward process equations, if the model perfectly predicts $x_0$, we can recover the noise prediction via the relationship above. Both parametrizations contain the same information and lead to the same reverse process.

\textbf{Advantage:} Direct $x_0$ prediction can be more intuitive since we're directly predicting the clean image. This can sometimes lead to faster convergence.

\textbf{Reverse Process with $x_0$ parametrization:}

During sampling, when we have $\hat{x}_0$, we can compute the posterior mean as:

$$\mu_t = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t$$
}
\includeanswer{q6a}

\subq{b}{10} Now implement your derived parametrization and train a model with the same configuration with your previous DDPM implementation with the only difference being the model parametrization. Include a grid of 16 samples and report your KID below. (Budget tip: You can compare the two parameterizations using shorter training runs, as long as they provide meaningful comparisons.)

\answerbox{}{
% Your answer here
\vspace{10cm} % comment this line out after you put in your answers
}
\includeanswer{q6b}

\subq{c}{3} Compare the performance of your new parametrization with that of the original parametrization. Share your findings below.

\answerbox{}{
% Your answer here
\vspace{5cm} % comment this line out after you put in your answers
}
\includeanswer{q6c}

\question{Sampling Steps Ablation}{10}

\subq{a}{10} DDPM typically uses 1000 timesteps for sampling, which is slow. How about using 
fewer steps? Report the KID scores for DDPM sampling algorithm with 100, 300, 500, 700, 900 steps 
and compare with your original sampling with 1000 steps. Include 1 sample for each number of 
steps to qualitatively show your comparisons as well.

\answerbox{}{

All of the time steps were trained for 15000 training cycles.

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Sampling Steps} & \textbf{KID Mean} & \textbf{KID Std} \\
        \midrule
        100 & NA & NA \\
        300 & 0.02324087 & 0.0007573081 \\
        500 & 0.009578767 & 0.0005155397 \\
        700 & 0.005084994 & 0.000333052 \\
        900 & 0.004992685 & 0.0003104516 \\
        1000 & 0.004848759 & 0.000285707 \\
        \bottomrule
    \end{tabular}
    \caption{KID scores vs.\ number of sampling steps}
\end{table}

\vspace{0.5em}

\noindent\textbf{Qualitative Comparison:}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sample_300steps.png}
    \caption{300 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sample_500steps.png}
    \caption{500 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sample_700steps.png}
    \caption{700 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sample_900steps.png}
    \caption{900 steps}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/sample_1000steps.png}
    \caption{1000 steps}
\end{figure}

\vspace{0.5em}

\noindent\textbf{Analysis:}

\begin{itemize}
    \item \textbf{100 steps:} Ignored since only was being generated
    
    \item \textbf{300 steps:} Dark faces with abrupt structure.
    
    \item \textbf{500 steps:} More coherent faces with noticeable aberrations.
    
    \item \textbf{700 steps:} Proper images have started appearing but still not the majority
    
    \item \textbf{900 steps:} Slight improvement with a reduction in improper faces
    
    \item \textbf{1000 steps:} Images now also show much clearer background with better faces but not always
\end{itemize}

\vspace{0.3em}

\noindent\textbf{Key Findings:}

\begin{itemize}
    \item \textbf{Steep quality gains up to 700 steps:} Reducing from 1000 to 700 steps (30\% fewer steps) only incurs a 4.9\% KID penalty, making this an excellent efficiency-quality tradeoff.
    
    \item \textbf{Diminishing returns beyond 700 steps:} Going from 700 to 1000 steps yields only 4.7\% KID improvement at 43\% additional computational cost.
    
    \item \textbf{900 and 1000 steps nearly equivalent:} Only 0.2\% KID difference between 900 and 1000 steps, suggesting the model has essentially converged by step 900.
    
    \item \textbf{Suggested configurations:}
    \begin{itemize}
        \item \textbf{Balanced:} 700 steps for near-baseline quality with 30\% speedup, just below the assignment's 0.005 threshold
        \item \textbf{Good quality:} 900 steps for essentially baseline performance at 10\% speedup
        \item \textbf{Diminishing returns:} 1000 steps not justified given minimal quality gain over 900 steps but still best is best
    \end{itemize}
\end{itemize}

}
\includeanswer{q7a}