
\hwpart{II}{Implement DDPM (25 points)}
Now it's time to build your first diffusion model! Take a good look at the starter code and start building! Remember, you are free to add, delete and modify any and all parts of the starter code to fit your preference.

\question{Building Intuition}{optional, 0}

Before diving into the full scale training, it is generally a good idea to build some intuitions of the algorithm with some toy examples as they are easier to debug.

\subq{a}{0} We have prepared a Jupyter notebook \textit{01\_1d\_playground.ipynb} for you to experiment with 1D toy data. This notebook contains some options of mixture of Gaussians and their visualizations, you can try out your algorithm first in this playground.

\subq{b}{0} Full scale training can be difficult to debug sometimes, and that's why in \textit{train.py} we also provide an argument flag, \textit{-\--overfit-single-batch}, for you to toggle an experiment where you overfit to a single batch of data for sanity checking. You should be able to iterate your model a lot faster with this experiment than full scale training.

\question{Implementation}{25}

Now implement DDPM for real! The starter code provides the skeleton, and your job is to fill in the core algorithm.
Remember: this is your codebase. Feel free to modify any part of the starter code to fit your preferences. Add helper functions, reorganize files, change the config structure... whatever helps. The only requirement is that your final code runs and produces good results.

The starter code provides a detailed guideline on which files are provided and which files are the ones that need your implementation. Go check it out!

\subq{a}{5} Train your DDPM model to convergence. You may use either the provided configs or your own. Report:
\begin{enumerate}
    \item Model size
    \item Batch size
    \item Total training iterations
    \item Training loss curve
    \item Compute cost (GPU hours)
\end{enumerate}
\answerbox{}{
% Your answer here
\textit{Note: I had initially ran this for 20,000 steps however later I resumed it until 40,000 from the checkpoint. Everything stays constant} \\
\textbf{1. Model Size:} 71,432,579 parameters

\textbf{2. Batch Size:} 128

\textbf{3. Total Training Iterations:} 20,000 and then 40,000

\textbf{4. Training Loss Curve:} 
Y\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/loss_0_20k.png}
        \caption{Loss curve: 0--20k steps}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/loss_20k_40k.png}
        \caption{Loss curve: 20k--40k steps (continued with cosine scheduler)}
    \end{subfigure}
    \caption{Training loss curves showing rapid convergence followed by stabilization}
\end{figure}

The model demonstrates healthy convergence:
\begin{itemize}
    \item \textbf{Phase 1 (0--20k):} Loss decreases sharply from $\sim 0.14$ to $\sim 0.02$
    \item \textbf{Phase 2 (20k--40k):} Loss stabilizes around $0.016$--$0.0175$ with minor fluctuations due to batch stochasticity and the cosine annealing schedule
\end{itemize}

The ``bumpiness'' in Phase 2 is expected and indicates the model is refining details rather than suffering from instability.

\textbf{5. Compute Cost:} 1 hour and 59 min for 0-20000 run and roughly 2 hours for 20000-40000 run in detached mode

\vspace{0.5em}

}
\includeanswer{q4a}

\subq{b}{10} Generate a grid of 16 samples from your trained model. Include this grid below.
\answerbox{}{
% Your answer here
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/samples_40000.png}
    \caption{16 generated face samples from DDPM model at step 40,000}
\end{figure}
}
\includeanswer{q4b}

\subq{c}{10} Evaluate your model KID with 1k samples and report your KID score (both mean and std). You should be able to obtain KID < 0.005 with single L40S GPU training for a few hours.
\answerbox{}{
\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Checkpoint} & \textbf{KID Mean} & \textbf{KID Std} \\
        \midrule
        20k steps & 0.004061 & 0.000267 \\
        40k steps (resumed from 20k) & 0.004676 & 0.000296 \\
        \bottomrule
    \end{tabular}
    \caption{KID evaluation scores at different training iterations}
\end{table}

\noindent\textbf{Result:} \textbf{Both checkpoints achieved KID $< 0.005$}

\vspace{0.3em}

\noindent The 20k checkpoint achieved the best KID score of \textbf{0.004061 $\pm$ 0.000267}. The slight increase at 40k iterations (0.004676) is likely due to the cosine learning rate scheduler.
}
\includeanswer{q4c}

\subq{d}{0} Provide a zip file of your code through Gradescope ``Homework 1 Code'' assignment. Make sure your code is runnable because we may run it to verify your results, and do not include any large files such as model checkpoints or dataset files. If you do not provide a zip file for your code, you will receive 0 point on Part II and Part IV of this homework.
\includeanswer{q4d}